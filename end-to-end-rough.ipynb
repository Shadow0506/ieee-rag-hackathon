{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pypdf import PdfReader\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    # Create a PdfReader object\n",
    "    reader = PdfReader(pdf_path)\n",
    "\n",
    "    # Initialize a string to hold all the extracted text\n",
    "    extracted_text = \"\"\n",
    "\n",
    "    # Loop through each page and extract text\n",
    "    for page in reader.pages:\n",
    "        extracted_text += page.extract_text()\n",
    "\n",
    "    # Print the extracted text\n",
    "    # with open('extracted_text.txt', 'w') as text_file:\n",
    "        # text_file.write(extracted_text)\n",
    "\n",
    "    return extracted_text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pdf_path = 'data/Finance_Project.pdf'\n",
    "text = extract_text_from_pdf(pdf_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Split document into 371 chunks.\n"
     ]
    }
   ],
   "source": [
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,\n",
    "    chunk_overlap=200,\n",
    "    length_function=len,\n",
    "    add_start_index=True,\n",
    ")\n",
    "\n",
    "chunks = text_splitter.split_text(text)\n",
    "print(f\"Split document into {len(chunks)} chunks.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bhavy\\AppData\\Local\\Temp\\ipykernel_14312\\3230921713.py:3: LangChainDeprecationWarning: The class `HuggingFaceEmbeddings` was deprecated in LangChain 0.2.2 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-huggingface package and should be used instead. To use it run `pip install -U :class:`~langchain-huggingface` and import as `from :class:`~langchain_huggingface import HuggingFaceEmbeddings``.\n",
      "  embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\", model_kwargs={\"trust_remote_code\":True})\n",
      "e:\\Hackathons\\IEEE\\venv\\Lib\\site-packages\\sentence_transformers\\cross_encoder\\CrossEncoder.py:13: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from tqdm.autonotebook import tqdm, trange\n"
     ]
    }
   ],
   "source": [
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "\n",
    "embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\", model_kwargs={\"trust_remote_code\":True}) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'your_file.pdf'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 7\u001b[0m\n\u001b[0;32m      4\u001b[0m pdf_path \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124myour_file.pdf\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m      5\u001b[0m documents \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m----> 7\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mpdfplumber\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mopen\u001b[49m\u001b[43m(\u001b[49m\u001b[43mpdf_path\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m pdf:\n\u001b[0;32m      8\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m i, page \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(pdf\u001b[38;5;241m.\u001b[39mpages):\n\u001b[0;32m      9\u001b[0m         text \u001b[38;5;241m=\u001b[39m page\u001b[38;5;241m.\u001b[39mextract_text()\n",
      "File \u001b[1;32me:\\Hackathons\\IEEE\\venv\\Lib\\site-packages\\pdfplumber\\pdf.py:92\u001b[0m, in \u001b[0;36mPDF.open\u001b[1;34m(cls, path_or_fp, pages, laparams, password, strict_metadata, unicode_norm, repair, gs_path, repair_setting)\u001b[0m\n\u001b[0;32m     90\u001b[0m     path \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m     91\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(path_or_fp, (\u001b[38;5;28mstr\u001b[39m, pathlib\u001b[38;5;241m.\u001b[39mPath)):\n\u001b[1;32m---> 92\u001b[0m     stream \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mpath_or_fp\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m     93\u001b[0m     stream_is_external \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m     94\u001b[0m     path \u001b[38;5;241m=\u001b[39m pathlib\u001b[38;5;241m.\u001b[39mPath(path_or_fp)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'your_file.pdf'"
     ]
    }
   ],
   "source": [
    "import pdfplumber\n",
    "\n",
    "# Extract text from the PDF with page numbers\n",
    "pdf_path = \"your_file.pdf\"\n",
    "documents = []\n",
    "\n",
    "with pdfplumber.open(pdf_path) as pdf:\n",
    "    for i, page in enumerate(pdf.pages):\n",
    "        text = page.extract_text()\n",
    "        # Split text into smaller chunks if needed\n",
    "        documents.append({\"text\": text, \"metadata\": {\"page_number\": i + 1}})\n",
    "\n",
    "# Now, add these documents to the Chroma database with the page number in metadata\n",
    "db.add_documents(documents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "db = Chroma.from_texts(\n",
    "    chunks, embeddings, persist_directory=\"./chroma\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_similar(query):\n",
    "    result = db.similarity_search_with_relevance_scores(query=query, k=1)\n",
    "    # return result\n",
    "    return [(doc.page_content, doc.metadata.get('source', 'Unknown')) for doc, _ in result]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(Document(metadata={}, page_content='6. Comparative  Volatility  Analysis  across  Time  Zones ………………………………... 3 3 \\n7. Recommendations  and Implications  .............................................................................. 35 \\n8. Conclusion  ........................................................................................................................ 37 \\n9. Bibliography .................................................................................................................... 39'), -0.0417235927438766)]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bhavy\\AppData\\Local\\Temp\\ipykernel_26576\\1372054378.py:2: UserWarning: Relevance scores must be between 0 and 1, got [(Document(metadata={}, page_content='6. Comparative  Volatility  Analysis  across  Time  Zones ………………………………... 3 3 \\n7. Recommendations  and Implications  .............................................................................. 35 \\n8. Conclusion  ........................................................................................................................ 37 \\n9. Bibliography .................................................................................................................... 39'), -0.0417235927438766)]\n",
      "  result = db.similarity_search_with_relevance_scores(query=query, k=1)\n"
     ]
    }
   ],
   "source": [
    "query = \"What are the main points of the document?\"\n",
    "relevant_chunks = get_similar(query)\n",
    "print(relevant_chunks)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the user query using only the relevant information provided to you in this prompt.\n",
    "\n",
    "Relevant context from documents:\n",
    "{relevant}\n",
    "\n",
    "user query: {query}\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "import groq"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "messages = [{\"role\": \"system\", \"content\": \"You are a helpful chatbot\"}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from groq import Groq\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "\n",
    "def ask_ques(user_query):\n",
    "    template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "    prompt = template.format(\n",
    "        relevant=get_similar(user_query),\n",
    "        query=user_query\n",
    "    )\n",
    "\n",
    "    client = Groq(\n",
    "        api_key=\"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\",\n",
    "    )\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=messages,\n",
    "        model=\"llama3-8b-8192\",\n",
    "    )\n",
    "\n",
    "    messages.pop()\n",
    "    messages.append({\"role\": \"user\", \"content\": \"user_query\"})\n",
    "    messages.append({\"role\": \"assistant\", \"content\": chat_completion.choices[0].message.content})\n",
    "\n",
    "    print(f\"Prompt: {prompt}\")\n",
    "    print(f\"\\n\\nResponse: {chat_completion.choices}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Prompt: Human: \n",
      "Answer the user query using only the relevant information provided to you in this prompt.\n",
      "\n",
      "Relevant context from documents:\n",
      "[('helping the market mature and become a more viable and stable asset class for investors.  \\n \\nThe Volatility  of Bitcoin  and Its Role as a Medium  of Exchange  and a Store of Value  \\n \\nThe paper titled \"The Volatility of Bitcoin and Its Role as a Medium of Exchange and a Store of Value\" \\noffers a thorough examination of Bitcoin\\'s extreme price fluctuations and their implications for Bitcoin\\'s', 'Unknown')]\n",
      "\n",
      "user query: Tell me about bitcoin volatility\n",
      "\n",
      "\n",
      "\n",
      "Response: [Choice(finish_reason='stop', index=0, logprobs=None, message=ChatCompletionMessage(content='According to the provided context, the paper titled \"The Volatility of Bitcoin and Its Role as a Medium of Exchange and a Store of Value\" offers a thorough examination of Bitcoin\\'s extreme price fluctuations, which are commonly referred to as volatility.', role='assistant', function_call=None, tool_calls=None))]\n"
     ]
    }
   ],
   "source": [
    "input = \"Tell me about bitcoin volatility\"\n",
    "ask_ques(input)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bakchodi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "\n",
    "embeddings = SentenceTransformerEmbeddings(model_name=\"all-MiniLM-L6-v2\", model_kwargs={\"trust_remote_code\":True})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'similarity_search_with_relevance_scores'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[97], line 2\u001b[0m\n\u001b[0;32m      1\u001b[0m query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mYour search query\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m----> 2\u001b[0m results \u001b[38;5;241m=\u001b[39m \u001b[43mdb\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msimilarity_search_with_relevance_scores\u001b[49m(query\u001b[38;5;241m=\u001b[39mquery, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m5\u001b[39m)\n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Extract the text and metadata (including page number)\u001b[39;00m\n\u001b[0;32m      5\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m result, score \u001b[38;5;129;01min\u001b[39;00m results:\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'similarity_search_with_relevance_scores'"
     ]
    }
   ],
   "source": [
    "query = \"Your search query\"\n",
    "results = db.similarity_search_with_relevance_scores(query=query, k=5)\n",
    "\n",
    "# Extract the text and metadata (including page number)\n",
    "for result, score in results:\n",
    "    context_text = result[\"text\"]\n",
    "    page_number = result[\"metadata\"].get(\"page_number\")\n",
    "    print(f\"Context: {context_text}\")\n",
    "    print(f\"Page: {page_number}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found in page 8: 8 must develop new tools and frameworks to better manage the risks associated with this emerging asset class. At the same time, regulators must work to provide clearer guidelines to ensure market stability and protect investors. Ultimately, a deeper understanding of cryptocurrency volatility will be essential in helping the market mature and become a more viable and stable asset class for investors. The Volatility of Bitcoin and Its Role as a Medium of Exchange and a Store of Value The paper titled \"The Volatility of Bitcoin and Its Role as a Medium of Exchange and a Store of Value\" offers a thorough examination of Bitcoin's extreme price fluctuations and their implications for Bitcoin's potential to function as a cur rency or store of value. Given Bitcoin's rising prominence in financial markets and its increasing adoption by both retail and institutional investors, the study aims to uncover how Bitcoin's volatility affects its utility in the world of finance. Bitcoin' s decentralized, digital nature has positioned it as a revolutionary asset, yet its excessive volatility raises questions about its feasibility as a medium of exchange and a long- term store of value. The research carefully analyzes Bitcoin’s price dynamics in comparison to traditional currencies and examines the role it could play in diversified investment portfolios. While the paper acknowledges Bitcoin’s potential as an asset class, it highlights the challenges posed by its significant short -term price in stability. Extreme Volatility of Bitcoin One of the central findings of the study is Bitcoin’s extreme volatility, which dwarfs that of traditional exchange rates. The paper highlights that Bitcoin’s price fluctuations are almost 10 times higher than those of major currency pairs such as USD/EUR or USD/JPY. This extreme volatility is largely driven by factors such as speculative trading, limited regulation, and the absence of a central authority governing Bitcoin’s supply. The decentralized nature of B itcoin allows for high levels of price manipulation and speculative investments, which can result in sudden and dramatic price changes. The research shows that Bitcoin’s extreme volatility presents a significant obstacle to its use as a stable medium of e xchange. Traditional currencies, such as the U.S. dollar or the Euro, exhibit much lower levels of volatility, allowing them to be used reliably for daily transactions. In contrast, Bitcoin's price can fluctuate wildly over short periods, making it difficult for merchants or consumers to use it as a stable medium of exchange. This volatility can result in prices that change dramatically between the time a transaction is initiated and the time it is completed, leading to added costs and risks for both buyers and sellers. The paper also highlights that Bitcoin's volatility poses challenges for its inclusion in investment portfolios. While Bitcoin's high returns may make it an attractive asset to some investors, its excessive volatility diminishes its r ole as a risk -diversifier. The authors argue that the high degree of uncertainty surrounding Bitcoin's price movements reduces its utility in minimizing overall portfolio risk. As a result, minimum\n",
      "Found in page 10: 10 Bitcoin’s Potential as a Store of Value While Bitcoin's volatility limits its role as a medium of exchange and a portfolio diversifier, the paper highlights its potential to serve as a long-term store of value. A store of value is an asset that maintains its value over time, allowing individuals to preserve wealth. Traditional stores of value include assets like gold, which have historically maintained their value even during periods of economic uncertainty. The authors argue that Bitcoin exhibits some characteristics of a store of value, particularly over longer time horizons. One of the key factors contributing to Bitcoin's potential as a store of value is its deflationary design. Unlike fiat currencies, which can be printed in unlimited quantities by central banks, Bitcoin has a fixed supply of 21 million coins. This limited supply creates a deflationary dynamic, as the increasing demand for Bitcoin is not matched by an increase in supply. Over the long term, this has resulted in rising Bitcoin prices, which suggests that Bitcoin could serve as a store of value for those willing to tolerate its short - term volatility. However, the study emphasizes that Bitcoin's excessive short -term volatility presents challenges to i ts role as a store of value in the near term. While Bitcoin has experienced significant price appreciation over longer time horizons, its value can fluctuate wildly over short periods. This volatility makes it difficult for individuals to rely on Bitcoin as a stable store of value for day -to-day transactions or short -term financial planning. As a result, Bitcoin's utility as a store of value may be limited to investors with a long -term horizon who are willing to weather the short -term price fluctuations. In conclusion, the paper \"The Volatility of Bitcoin and Its Role as a Medium of Exchange and a Store of Value\" provides valuable insights into the challenges posed by Bitcoin’s extreme volatility. The study shows that Bitcoin’s price fluctuations, which ar e nearly 10 times higher than those of traditional currencies, limit its potential as a medium of exchange. The volatility makes daily transactions risky and expensive, and discourages its use as a stable currency. Additionally, Bitcoin's excessive volatil ity reduces its effectiveness as a portfolio diversifier, leading to low or zero allocations in risk -averse investment portfolios. Despite these challenges, the research also highlights Bitcoin's potential as a long -term store of value. Its deflationary design and increasing long -term price trends suggest that Bitcoin could serve as an alternative to assets like gold for investors with a long -term perspective. However, in the near term, its extreme price volatility continues to hinder its role as both a currency and a store of value. The findings of this paper underscore the need for further research and regulation to better understand Bitcoin’s place in the global financial system, and to address the challenges posed by its volatility. Cryptocurrencies and Stablecoins: A High -Frequency Analysis\n",
      "Found in page 9: suggests that Bitcoin's volatility is not well integrated into the global foreign exchange (FX) market. Unlike traditi onal currencies, which are traded in highly regulated FX markets with deep liquidity, Bitcoin operates in a relatively fragmented and unregulated environment. This lack of integration with the FX market further limits Bitcoin's role as a currency and a portfolio diversifier.\n",
      "Found in page 30: 30 mid-2021, reaching over 1.6, indicative of the substantial price fluctuations characteristic of the cryptocurrency market during that period. Ethereum follows a similar trend but exhibits slightly lower volatility than Bitcoin, with its peak s occurring later in 2021 and then again in 2023. On the other hand, the volatility of gold, represented by the blue line, and NIFTY, indicated in light blue, remains relatively stable and low in comparison. Both traditional assets show a consistent volati lity pattern, with gold demonstrating only minor fluctuations around 0.2, while NIFTY exhibits similar stability. The stark contrast in volatility between cryptocurrencies and traditional assets emphasizes the inherent risks associated with investing in digital currencies. This volatility is a crucial consideration for investors and market analysts, as it reflects the uncertain and speculative nature of the cryptocurrency market, especially in times of economic instability or regulatory changes. Overall, the data suggests that while cryptocurrencies can provide high -risk opportunities for returns, they are significantly more volatile than traditional investments, which may appeal to risk-tolerant investors but require careful consideration from those looki ng for stability. From the visualization, we can observe several important trends and patterns. The test set target volatility , represented by a solid blue line, fluctuates throughout the observed period, indicating varying levels of market volatility. The graph shows notable peaks and troughs in volatility, particularly evident during critical market events or fluctuations. In contrast, the current daily volatility , depicted with a dashed line, appears to track closely with the target volati lity, suggesting that real- time assessments of volatility are generally aligned with historical data. Additionally, the forecasted volatility, represented in yellow, indicates the expected volatility based on the model's predictions. This forecast demonstr ates a blend of predictive accuracy and responsiveness to market movements, which can be essential for investors seeking to manage\n",
      "Found in page 9: 9 Another important aspect of the research is its examination of Bitcoin’s ability to serve as a medium of exchange. In traditional financial systems, a medium of exchange is expected to provide price stability to ensure that transactions can be conducted with minimal risk or confusion. This is one of the primary functions of currency in the global ec onomy. However, the study finds that Bitcoin’s extreme price volatility undermines its role as a reliable medium of exchange. The paper emphasizes that the fluctuating price of Bitcoin creates significant difficulties for merchants and consumers alike. Fo r instance, if a merchant sets a price for a product in Bitcoin, the value of the Bitcoin received can change dramatically between the time the transaction is initiated and when it is completed. This introduces uncertainty and risk into the transaction, potentially resulting in financial losses for either party. Merchants are often forced to constantly update prices in response to Bitcoin's volatility, which creates inefficiencies and adds to the overall cost of conducting transactions. Moreover, the lack of price stability makes Bitcoin a poor choice for everyday transactions, as consumers cannot rely on its value remaining constant. In addition to these practical challenges, Bitcoin’s volatility also discourages its use as a currency from an economic standpoint. Traditional currencies are subject to government regulations and are backed by central banks, which can intervene to stabilize exchange rates and ensure that the currency maintains its value. Bitcoin, on the other hand, operates in a decentrali zed environment with no central authority to regulate its price. This lack of regulation contributes to the extreme price fluctuations observed in the cryptocurrency market, further complicating its role as a medium of exchange. Bitcoin and Portfolio Dive rsification The paper also investigates Bitcoin's potential role in investment portfolios, specifically its ability to serve as a risk -diversifier. In theory, Bitcoin’s high returns should make it an attractive asset for investors seeking to maximize profits. However, the study reveals that Bitcoin’s excessive volatility diminishes its effectiveness as a portfolio diversifier. In financial theory, a risk -diversifier is an asset that reduces the overall risk of a portfolio by offering returns that are not strongly correlated with the other assets in the portfolio. While Bitcoin does offer returns that are uncorrelated with traditional asset classes, its extreme volatility makes it unsuitable for inclusion in portfolios that prioritize risk management. The authors explain that in minimum variance portfolios, which aim to minimize risk while maintaining acceptable returns, Bitcoin often receives very low or zero portfolio weights. This is because the volatility introduced by Bitcoin can offset any potentia l gains, leading to an increase in overall portfolio risk. Despite the appeal of Bitcoin's high returns, the risk it introduces into a portfolio often outweighs its potential benefits. As a result, most investors seeking to minimize risk are likely to avoi d including Bitcoin in their portfolios, or to allocate only a small portion of their portfolio to this asset. Furthermore, the research\n"
     ]
    }
   ],
   "source": [
    "from pypdf import PdfReader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "\n",
    "# Initialize the sentence transformer model for embeddings\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Initialize Chroma client\n",
    "client = chromadb.Client()\n",
    "\n",
    "# Create or load a Chroma collection\n",
    "collection_name = \"pdf_chunks\"\n",
    "collection = client.get_or_create_collection(name=collection_name)\n",
    "\n",
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts text from each page of the PDF and returns it as a list of tuples,\n",
    "    where each tuple contains the text of a page and its corresponding page number.\n",
    "    \"\"\"\n",
    "    text_per_page = []\n",
    "    \n",
    "    # Open the PDF file\n",
    "    reader = PdfReader(pdf_path)\n",
    "    \n",
    "    # Loop through all pages and extract text\n",
    "    for page_num, page in enumerate(reader.pages, start=1):\n",
    "        text = page.extract_text()  # Extract text from the page\n",
    "        if text:  # Check if the page has any text\n",
    "            text_per_page.append([text, page_num])  # Append tuple (text, page number)\n",
    "    \n",
    "    return text_per_page\n",
    "\n",
    "def chunk_text(text, chunk_size=500):\n",
    "    \"\"\"\n",
    "    Splits the text into smaller chunks of the specified size.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    return [' '.join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]\n",
    "\n",
    "def add_text_chunks_to_chroma(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF, breaks it into chunks, generates embeddings,\n",
    "    and stores them in the Chroma database.\n",
    "    \"\"\"\n",
    "    pdf_text = extract_text_from_pdf(pdf_path)\n",
    "    \n",
    "    chunks = []\n",
    "    metadata = []\n",
    "    ids = []\n",
    "    \n",
    "    for i, (text, page_num) in enumerate(pdf_text):\n",
    "        # Chunk the text into smaller pieces\n",
    "        chunked_text = chunk_text(text)\n",
    "        chunks.extend(chunked_text)\n",
    "        \n",
    "        # Store metadata with page number for each chunk\n",
    "        metadata.extend([{\"page_num\": page_num}] * len(chunked_text))\n",
    "        \n",
    "        # Generate unique IDs for each chunk\n",
    "        ids.extend([f\"chunk_{i}_{j}\" for j in range(len(chunked_text))])\n",
    "    \n",
    "    # Generate embeddings for all chunks\n",
    "    embeddings = embedder.encode(chunks, convert_to_tensor=False)\n",
    "    \n",
    "    # Add the chunks, metadata, embeddings, and IDs to the Chroma collection\n",
    "    collection.add(\n",
    "        ids=ids,\n",
    "        documents=chunks,\n",
    "        metadatas=metadata,\n",
    "        embeddings=embeddings\n",
    "    )\n",
    "\n",
    "def search_in_chroma(query):\n",
    "    \"\"\"\n",
    "    Performs a similarity search for the given query using the Chroma database.\n",
    "    \"\"\"\n",
    "    # Generate embedding for the query\n",
    "    query_embedding = embedder.encode(query, convert_to_tensor=False)\n",
    "    \n",
    "    # Perform similarity search in Chroma\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=5\n",
    "    )\n",
    "    \n",
    "    # Return the top 5 results along with their metadata\n",
    "    return results[\"documents\"], results[\"metadatas\"]\n",
    "\n",
    "# Example usage\n",
    "pdf_path = \"data/Finance_Project.pdf\"  # Replace this with the path to your PDF file\n",
    "question = \"Tell me something about bitcoin volatility\"  # Replace this with your question\n",
    "\n",
    "# Step 1: Add chunks of the PDF to Chroma\n",
    "add_text_chunks_to_chroma(pdf_path)\n",
    "\n",
    "# Step 2: Search for the question in the Chroma database\n",
    "documents, metadatas = search_in_chroma(question)\n",
    "\n",
    "# Step 3: Print out the results with page numbers\n",
    "for doc, meta in zip(documents[0], metadatas[0]):\n",
    "    print(f\"Found in page {meta['page_num']}: {doc}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ask_for_questions():\n",
    "    template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "    \n",
    "    # Retrieve top 5 relevant chunks\n",
    "    relevant_chunks = get_similar(\"Generate questions from this document\")\n",
    "\n",
    "    # Format the prompt with the relevant information\n",
    "    prompt = template.format(\n",
    "        relevant=relevant_chunks,\n",
    "        query=\"Generate questions from this document\"\n",
    "    )\n",
    "\n",
    "    # Call the LLM API to generate questions\n",
    "    client = Groq(\n",
    "        api_key=\"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\",\n",
    "    )\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=messages,\n",
    "        model=\"llama-3.1-8b-instant\",\n",
    "    )\n",
    "\n",
    "    # Get and print the generated questions\n",
    "    response = chat_completion.choices[0].message.content\n",
    "    print(f\"Generated Questions:\\n{response}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\bhavy\\AppData\\Local\\Temp\\ipykernel_14312\\3116442797.py:2: UserWarning: Relevance scores must be between 0 and 1, got [(Document(metadata={}, page_content='5. Parth Kushwaha  : 23103064  2  Table  of Contents  \\n1. Abstract  ...............................................................................................................................3  \\n2. Introduction  ....................................................................................................................... 4 \\n3. Review  of Literature  ......................................................................................................... 5 \\n○ Need  of the Study'), -0.15770025590306203)]\n",
      "  result = db.similarity_search_with_relevance_scores(query=query, k=1)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Generated Questions:\n",
      "Here are some questions that can be generated from the provided document:\n",
      "\n",
      "1. What is the main subject of the study mentioned in the table of contents?\n",
      "2. How many chapters does the study have, according to the table of contents?\n",
      "3. What is the title of Abstract chapter in the study?\n",
      "4. What is the number of the chapter in the Introduction section?\n",
      "5. What is the chapter that reviews the Literature in the study?\n"
     ]
    }
   ],
   "source": [
    "ask_for_questions()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_text_from_pdf(pdf_path):\n",
    "    \"\"\"\n",
    "    Extracts text from each page of the PDF and returns it as a list of tuples,\n",
    "    where each tuple contains the text of a page and its corresponding page number.\n",
    "    \"\"\"\n",
    "    text_per_page = []\n",
    "    \n",
    "    # Open the PDF file\n",
    "    reader = PdfReader(pdf_path)\n",
    "    \n",
    "    # Loop through all pages and extract text\n",
    "    for page_num, page in enumerate(reader.pages, start=1):\n",
    "        text = page.extract_text()  # Extract text from the page\n",
    "        if text:  # Check if the page has any text\n",
    "            text_per_page.append((text, page_num))  # Append tuple (text, page number)\n",
    "    \n",
    "    return text_per_page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import fitz  # PyMuPDF\n",
    "from pypdf import PdfReader\n",
    "import torch\n",
    "from PIL import Image\n",
    "from transformers import BlipProcessor, BlipForConditionalGeneration\n",
    "import io\n",
    "import groq\n",
    "from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "from langchain_community.embeddings import SentenceTransformerEmbeddings\n",
    "from langchain_community.vectorstores import Chroma\n",
    "import os\n",
    "from groq import Groq\n",
    "from langchain.prompts import ChatPromptTemplate\n",
    "from pypdf import PdfReader\n",
    "from sentence_transformers import SentenceTransformer\n",
    "import chromadb\n",
    "\n",
    "# Initialize the sentence transformer model for embeddings\n",
    "embedder = SentenceTransformer('all-MiniLM-L6-v2')\n",
    "\n",
    "# Initialize Chroma client\n",
    "client = chromadb.Client()\n",
    "\n",
    "# Create or load a Chroma collection\n",
    "collection_name = \"pdf_chunks\"\n",
    "collection = client.get_or_create_collection(name=collection_name)\n",
    "\n",
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the user query using only the relevant information provided to you in this prompt. Write questions inside <> wherever you think you might need more information and it will be provided to you. If the 'Your Response:' part is empty then you have to generate the questions, if it is not empty then you have to replace the questions with new information if it is of any use, otherwise just remove those questions\n",
    "\n",
    "Your Response:\n",
    "{your_response}\n",
    "\n",
    "Relevant context from documents:\n",
    "{relevant}\n",
    "\n",
    "user query: {query}\n",
    "\"\"\"\n",
    "\n",
    "messages = [{\"role\": \"system\", \"content\": \"You are a helpful chatbot\"}]\n",
    "\n",
    "def get_stuff(text, image_text, user_query, messages):\n",
    "    # Step 1: Add chunks of the PDF to Chroma\n",
    "    add_text_chunks_to_chroma(text, image_text)\n",
    "\n",
    "    # Step 2: Search for the question in the Chroma database\n",
    "    documents, metadatas = search_in_chroma(user_query)\n",
    "\n",
    "    response1 = ask_ques(documents, user_query, messages)\n",
    "    print(response1)\n",
    "\n",
    "    # Step 3: Print out the results with page numbers\n",
    "    # for doc, meta in zip(documents[0], metadatas[0]):\n",
    "    #     print(f\"Found in page {meta['page_num']}: {doc}\")\n",
    "\n",
    "def chunk_text(text, chunk_size=500):\n",
    "    \"\"\"\n",
    "    Splits the text into smaller chunks of the specified size.\n",
    "    \"\"\"\n",
    "    words = text.split()\n",
    "    return [' '.join(words[i:i+chunk_size]) for i in range(0, len(words), chunk_size)]\n",
    "\n",
    "def add_text_chunks_to_chroma(text, image_text):\n",
    "    \"\"\"\n",
    "    Extracts text from a PDF, breaks it into chunks, generates embeddings,\n",
    "    and stores them in the Chroma database.\n",
    "    \"\"\"\n",
    "    pdf_text = extract_text_from_pdf(\"data/Finance_Project.pdf\")\n",
    "    \n",
    "    chunks = []\n",
    "    metadata = []\n",
    "    ids = []\n",
    "    \n",
    "    for i, (text, page_num) in enumerate(pdf_text):\n",
    "        # Chunk the text into smaller pieces\n",
    "        chunked_text = chunk_text(text)\n",
    "        chunks.extend(chunked_text)\n",
    "        \n",
    "        # Store metadata with page number for each chunk\n",
    "        metadata.extend([{\"page_num\": page_num}] * len(chunked_text))\n",
    "        \n",
    "        # Generate unique IDs for each chunk\n",
    "        ids.extend([f\"chunk_{i}_{j}\" for j in range(len(chunked_text))])\n",
    "    \n",
    "    chunks = chunks + image_text\n",
    "\n",
    "    # Generate embeddings for all chunks\n",
    "    embeddings = embedder.encode(chunks, convert_to_tensor=False)\n",
    "    \n",
    "    # Add the chunks, metadata, embeddings, and IDs to the Chroma collection\n",
    "    collection.add(\n",
    "        ids=ids,\n",
    "        documents=chunks,\n",
    "        metadatas=metadata,\n",
    "        embeddings=embeddings\n",
    "    )\n",
    "\n",
    "def search_in_chroma(query):\n",
    "    \"\"\"\n",
    "    Performs a similarity search for the given query using the Chroma database.\n",
    "    \"\"\"\n",
    "    # Generate embedding for the query\n",
    "    query_embedding = embedder.encode(query, convert_to_tensor=False)\n",
    "    \n",
    "    # Perform similarity search in Chroma\n",
    "    results = collection.query(\n",
    "        query_embeddings=[query_embedding],\n",
    "        n_results=5\n",
    "    )\n",
    "    \n",
    "    # Return the top 5 results along with their metadata\n",
    "    # return results[\"documents\"], results[\"metadatas\"]\n",
    "    return results[\"documents\"], results[\"metadatas\"]\n",
    "\n",
    "def ask_ques(results, user_query, messages):\n",
    "    template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "    prompt = template.format(\n",
    "        relevant=results,\n",
    "        query=user_query\n",
    "    )\n",
    "\n",
    "    client = Groq(\n",
    "        api_key=\"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\",\n",
    "    )\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=messages,\n",
    "        model=\"llama3-8b-8192\",\n",
    "    )\n",
    "\n",
    "    messages.pop()\n",
    "    messages.append({\"role\": \"user\", \"content\": user_query})\n",
    "    messages.append({\"role\": \"assistant\", \"content\": chat_completion.choices[0].message.content})\n",
    "\n",
    "    return chat_completion.choices[0].message.content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_text = ['Image 1.1: education cart wait but why',\n",
    " 'Image 2.1: the world is not a water - free environment',\n",
    " 'Image 3.1: children education a little more thane',\n",
    " 'Image 4.1: a woman is writing on a piece of paper',\n",
    " 'Image 5.1: a child sitting on the floor with a book and pencil',\n",
    " 'Image 6.1: a piece of paper with a hand holding a pen',\n",
    " 'Image 7.1: classroom culture in order to achieve learning on time and movement',\n",
    " 'Image 7.2: a col of images of children in a classroom',\n",
    " 'Image 8.1: a col of images of children in a classroom',\n",
    " 'Image 9.1: mental welling assessment - mental welling assessment',\n",
    " 'Image 10.1: a col of photos of children in different places',\n",
    " 'Image 11.1: impact road we believe that a metaphor is not an answer',\n",
    " 'Image 12.1: two young boys writing in front of a book',\n",
    " 'Image 13.1: a graph shows the percentage of children who have been diagnosed',\n",
    " \"Image 14.1: education can't wait for the future\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'text' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[38;5;28mprint\u001b[39m(get_stuff(\u001b[43mtext\u001b[49m, image_text, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mTell me something about bitcoin volatility\u001b[39m\u001b[38;5;124m\"\u001b[39m, messages))\n",
      "\u001b[1;31mNameError\u001b[0m: name 'text' is not defined"
     ]
    }
   ],
   "source": [
    "print(get_stuff(text, image_text, \"Tell me something about bitcoin volatility\", messages))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "PROMPT_TEMPLATE = \"\"\"\n",
    "Answer the user query using only the relevant information provided to you in this prompt. \n",
    "Write questions inside <> wherever you think you might need more information and it will be provided to you.\n",
    "If the 'Your Response:' part is empty then you have to generate the questions, if it is not empty then you have to replace the questions with new information if it is of any use, otherwise just remove those questions.\n",
    "\n",
    "Your Response:\n",
    "{your_response}\n",
    "\n",
    "Relevant context from documents:\n",
    "{relevant}\n",
    "\n",
    "user query: {query}\n",
    "\"\"\"\n",
    "\n",
    "def ask_ques(results, user_query, messages, previous_response=\"\"):\n",
    "    \"\"\"\n",
    "    This function generates a response and potential clarifying questions.\n",
    "    If previous_response is empty, it generates questions. Otherwise, it integrates new context.\n",
    "    \"\"\"\n",
    "    template = ChatPromptTemplate.from_template(PROMPT_TEMPLATE)\n",
    "    \n",
    "    # Format prompt with the previous response and relevant context\n",
    "    prompt = template.format(\n",
    "        relevant=results,  # Pass relevant chunks or results from Chroma\n",
    "        query=user_query,\n",
    "        your_response=previous_response  # If empty, questions will be generated\n",
    "    )\n",
    "\n",
    "    client = Groq(\n",
    "        api_key=\"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\",\n",
    "    )\n",
    "\n",
    "    messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "\n",
    "    # Generate chat completion with Groq API\n",
    "    chat_completion = client.chat.completions.create(\n",
    "        messages=messages,\n",
    "        model=\"llama3-8b-8192\",\n",
    "    )\n",
    "\n",
    "    # Get the model's response (which may include questions)\n",
    "    response_content = chat_completion.choices[0].message.content\n",
    "\n",
    "    # Remove the last user prompt and update messages with new context\n",
    "    messages.pop()\n",
    "    messages.append({\"role\": \"user\", \"content\": user_query})\n",
    "    messages.append({\"role\": \"assistant\", \"content\": response_content})\n",
    "\n",
    "    return response_content\n",
    "\n",
    "def extract_questions(response_content):\n",
    "    \"\"\"\n",
    "    Extract questions from the model's response. Questions are inside <>.\n",
    "    \"\"\"\n",
    "    questions = []\n",
    "    start = response_content.find('<')\n",
    "    \n",
    "    while start != -1:\n",
    "        end = response_content.find('>', start)\n",
    "        if end != -1:\n",
    "            question = response_content[start+1:end]\n",
    "            questions.append(question)\n",
    "            start = response_content.find('<', end)\n",
    "        else:\n",
    "            break\n",
    "    \n",
    "    return questions\n",
    "\n",
    "def process_questions_and_search(questions):\n",
    "    \"\"\"\n",
    "    Uses the questions to perform a similarity search in Chroma.\n",
    "    \"\"\"\n",
    "    search_results = []\n",
    "    for question in questions:\n",
    "        # Perform vector similarity search using the extracted questions\n",
    "        documents, metadatas = search_in_chroma(question)\n",
    "        search_results.append((documents, metadatas))\n",
    "    \n",
    "    # Combine all search results into a single list\n",
    "    combined_documents = [doc for docs, _ in search_results for doc in docs]\n",
    "    return combined_documents\n",
    "\n",
    "def generate_final_response(relevant_context, user_query, previous_response, messages):\n",
    "    \"\"\"\n",
    "    Generate the final response by providing the previous response and new context from similarity search.\n",
    "    \"\"\"\n",
    "    final_response = ask_ques(relevant_context, user_query, messages, previous_response)\n",
    "    return final_response\n",
    "\n",
    "# Main flow function\n",
    "def get_stuff(text, image_text, user_query, messages):\n",
    "    # Step 1: Add chunks of the PDF to Chroma\n",
    "    add_text_chunks_to_chroma(text, image_text)\n",
    "\n",
    "    # Step 2: Search for the initial query in the Chroma database\n",
    "    documents, metadatas = search_in_chroma(user_query)\n",
    "    \n",
    "    # Step 3: Generate the initial model response with potential questions\n",
    "    initial_response = ask_ques(documents, user_query, messages)\n",
    "    \n",
    "    # Step 4: Extract the questions from the model's response\n",
    "    questions = extract_questions(initial_response)\n",
    "    \n",
    "    if questions:\n",
    "        # Step 5: Use the questions to perform similarity search\n",
    "        new_context = process_questions_and_search(questions)\n",
    "        \n",
    "        # Step 6: Generate the final response using the new context and the initial response\n",
    "        final_output = generate_final_response(new_context, user_query, initial_response, messages)\n",
    "        \n",
    "        return final_output\n",
    "    else:\n",
    "        # If no questions were generated, return the initial response\n",
    "        return initial_response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
